---
title: "651 class"
author: "Siyu Zou"
date: "2023-09-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
tq<-qexp(seq(0.01,0.99,length=100), 1/5)
plot(tq)

union(x,y) #æ±‚å¹¶é›†

intersect(x,y) #æ±‚äº¤é›†

setdiff(x,y) #æ±‚å±äºxè€Œä¸å±äºyçš„æ‰€æœ‰å…ƒç´ 

setequal(x,y) #åˆ¤æ–­xä¸yæ˜¯å¦ç›¸ç­‰

a %in% y #åˆ¤æ–­aæ˜¯å¦ä¸ºyä¸­çš„å…ƒç´ 

choose(n, k) #nä¸ªé‡Œé¢å–kä¸ªçš„ç»„åˆæ•°

combn(x,n) #xä¸­çš„å…ƒç´ æ¯æ¬¡å–nä¸ªçš„æ‰€æœ‰ç»„åˆ

combn(x,n,f) #å°†è¿™äº›ç»„åˆç”¨äºæŒ‡å®šå‡½æ•°f
```

# Lecture 1 & 2
## Probability calculus
```{r birthday problem}
n=1:364
pn=n
for (i in 1:364) {
  pn[i]<- 1-prod(365:(365-i+1))/365^i
  }

pn[23]  # 50%

pn[57]  # 99%
```

```{r ploting birthday problem}
plot(
  n[1:100],
  pn[1:100],
  type = "p",
  pch = 19, 
  col = "red",
  lwd = 3,
  xlab = "Number of people in the room",
  ylab = "Probability of at least two same-day birthdays",
  cex.lab = 1.5, cex.axis = 1.5, col.axis = "black"
)


```


### sample
```{r Monte Holl problem}

# help on sample
?sample
# a random permutation
x <- sample (1:6)  #  the default for size == the first argument, so that sample(x) generates a random permutation of the elements of x (or 1:x).

# sampling with replacement
x <- sample ( 1:6, 10, replace=T )  # size = 10

# how many are equal to 3?
sum ( x == 3 )

# nonparametric bootstrap
x <- sample ( 1:10, 10, replace=T )

# 100 Bernoulli trials
sample(c(0,1), 100, replace = TRUE)

```

### Can you write an R code to simulate the Monte Hall problem?
the answer is in Lab 2 notes


## Probability mass function (PMF) for discrete variable

### Bernoulli & Binomial (sum of Bernoulli)
```{r Bernoulli example}
?rbinom
# rbinom(n, size, prob)
# n: number of observations. If length(n) >1, the length is taken to be the number required.
# size: number of trails (0 or more)
# Let X be the result of a single coin flip where X = 0 represents tails and X = 1 represents heads

# Bernoulli: 1æ¬¡å®éªŒ
rbinom(1,1,0.5)
rbinom(20, 1, 0.3) #  1ä¸ªç¡¬å¸(size), æŠ›20æ¬¡(n)ï¼Œæ¯æ¬¡æ¦‚ç‡0.3 (p)
rbinom(20,1,seq(0,1,length=20))  #  1ä¸ªç¡¬å¸(size), æŠ›20æ¬¡(n)ï¼Œæ¯æ¬¡æ¦‚ç‡ ä»0åˆ°1å¢åŠ  (p)

# Binomial : n æ¬¡Bernoulliå®éªŒï¼Œ sizeï¼šæ€»å…±å¤šå°‘ä¸ªç¡¬å¸
# Simulate 15 independent Binomial(10,.2)
rbinom(15,size=10,prob=0.2)  # 10ä¸ªç¡¬å¸ï¼ˆsizeï¼‰ï¼ŒæŠ›15æ¬¡ï¼ˆnï¼‰ï¼Œæ¯æ¬¡æ¦‚ç‡0.2
# Simulate 15 independent Binomial(10,p)
# First p=0.1, last p=0.9
rbinom(15,size=10,prob=seq(0.1,0.9,length=15))
# Simulate 15 independent Binomial(n,p)
# First (n,p)=(1,0.1), last (n,p)=(15,0.9)
rbinom(15,size=1:15,prob=seq(0.1,0.9,length=15))  # 1-15ä¸ªç¡¬å¸ï¼ˆsizeï¼‰ï¼ŒæŠ›15æ¬¡ï¼ˆnï¼‰ï¼Œæ¯æ¬¡æ¦‚ç‡0.1-0.9
```

Suppose a man has 8 children, 7 of which are girls and none are twins
 If each gender has an independent 50% probability for each birth, what's the probability of getting 7 or more girls out of 8 births?
```{r}
# distribution function
pbinom(6, 8, 0.5, lower.tail = FALSE)  # cdf (cumulative distribution)

# density function
dbinom(6,8,0.5)  # pmf, pdf (density)
choose(8,6)*0.5^8

# quantile function 
# qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE) 
```
This calculation is an example of a p-value: the probability, under the null hypothesis, of getting a result as extreme or more extreme than the one actually obtained. 
The p-value or probability value is the probability of obtaining test results at least as extreme as the results actually observed during the test, assuming that the null hypothesis is correct.

# Normal distribution
```{r}
# Probability of at most 1Ïƒ eviation
pnorm(1)-pnorm(-1)  # 0.68
# Probability of at most 2Ïƒ deviation
pnorm(2)-pnorm(-2)  # 0.954
# Probability of at most 3Ïƒdeviation
pnorm(3)-pnorm(-3)  # 0.997

pnorm(-1.28, mean = 0, sd = 1, lower.tail = TRUE)  # 10th percentiles
pnorm(-1.645, mean = 0, sd = 1, lower.tail = TRUE)  # 5th percentiles
pnorm(-1.96, mean = 0, sd = 1, lower.tail = TRUE)  # 2.5th percentiles
pnorm(-2.33, mean = 0, sd = 1, lower.tail = TRUE)  # 1th percentiles

# symmetry
pnorm(1.28, mean = 0, sd = 1, lower.tail = TRUE)  # 90th percentiles
pnorm(1.645, mean = 0, sd = 1, lower.tail = TRUE)  # 95th percentiles
pnorm(1.96, mean = 0, sd = 1, lower.tail = TRUE)  # 97.5th percentiles
pnorm(2.33, mean = 0, sd = 1, lower.tail = TRUE)  # 99th percentiles

```

What is the 95th percentile of a N(Î¼,Ïƒ2) distribution?
```{r qnorm}
qnorm(0.10, mean = 0, sd = 1)  # 10% percentile's x = -1.28
qnorm(0.90,mean = 0, sd = 1 )  # 90% percentile' x = 1.28
qnorm(0.95,mean = 0, sd = 1 )  # 90% percentile' x = 1.645
qnorm(0.975,mean = 0, sd = 1 )  # 90% percentile' x = 1.96
qnorm(0.99,mean = 0, sd = 1 )  # 90% percentile' x = 2.33

```

### Poisson
```{r Poisson example}
x <- 1:100
lambda = 20
plot(x, dpois(x, lambda),type="p",pch=19,
col="blue",lwd=3, xlab="Number of patients",
ylab="Probability",cex.lab=1.5,cex.axis=1.5,
col.axis="blue")


?dpois

# sampling the Poisson distribution 
# A sample of 15 independent days with average number of patients = 20
rpois(15, lambda=20)

# Reconstructing PMF from samples
y <- rpois(10000, lambda=20)
py=rep(0,100)
for (i in 1:100) {
  py[i] <- sum(y==i)/10000 
}

# Add lines to the PMF plot of the Poisson distribution
plot(x, dpois(x, lambda),type="p",pch=19,
col="blue",lwd=3, xlab="Number of patients",
ylab="Probability",cex.lab=1.5,cex.axis=1.5,
col.axis="blue") + 
lines(1:100,py,col="red",lwd=3)

```


## Probability density function for continuous variable
Whatâ€™s the probability that a randomly selected person from this distribution survives more than 6 years?
f(x) = (e^-x/5)/5
P(X>=6) = $\int_{0}^{Inf}e^-t/5 /5 dt$

```{r exponential}
pexp(6, 1/5, lower.tail = FALSE)
x <- 1:100
plot (dexp(x,1/5 ))
pexp(6, 1/5) - pexp(5, 1/5)
```

Double exponential
```{r double exponential}
x <- seq(-5,5,length(101))
fx <- exp(-abs(x))/2
plot(x, fx, type = "l", col = "lightblue", lwd = 3,
     xlab = "error", ylab = "PDF", cex.lab = 1.5,
     cex.axis = 1.5, col.axis = "blue"
     )
```
consider a more general version of f(.)
f(x) = $1/2Ïƒ e^(-|x-Î¼|/Ïƒ)$
is this a valid density?
Plotting double exponentials with mean Î¼ = 2 and Ïƒ = 1, 2, 4
```{r double exponentials Î¼ = 2}
# construct the empirical pdf from 100 simulated observation
mu <- 2
sigma = 2
# install.packages("vgam")
# library("vgam")
y = replace(100, mu, sigma)
hist(y, breaks = 20, probability = TRUE, xlab = "error")

# Super impose the theoretical pdf
x=seq(mu-7,mu+7,length=101)
fx=exp(-abs(x-mu)/sigma)/(2*sigma)
plot(x,fx,col="orange",lwd=3)

```

# CDF Cumulative distribution function

# Quantile
What is the 25th percentile of the exponential survival distribution considered before??
â€¢ We want to solve (for x)
.25 = F(x)
= 1 âˆ’ eâˆ’x/5
resulting in the solution x = âˆ’log(.75) Ã— 5 â‰ˆ 1.44
â€¢ Therefore, 25% of the subjects from this population live less than 1.44 years
```{r quantile}
# R can approximate exponential quantiles for you
qexp( p = 0.25, 1/5)  # quartile
qexp( p = 1/3, 1/5)  # tertile
```


# QQ plot
Calculate the quantiles of an exp(5) distribution
```{r qq}
tq<-qexp(seq(0.01,0.99,length=100), 1/5)
plot(tq)
```

How do they compare to the empirical quantiles?
```{r }
x1<-rexp(30, 1/5)
x2<-rexp(30, 1/5)
x3<-rexp(30, 1/5)
eq1<-quantile(x1,seq(0.01,0.99,length=100))
eq2<-quantile(x2,seq(0.01,0.99,length=100))
eq3<-quantile(x3,seq(0.01,0.99,length=100))
```

Plot theoretical versus empirical quantiles (QQ plots)
```{r qq plot}
library(ggplot2)
data <- data.frame( eq1, eq2, eq3, tq )

library(tidyverse)
data %>%
  ggplot() +
  geom_point(aes(tq, eq1 )) +
  geom_point(aes(tq,eq2),method = "lm" ) +
  geom_point(aes(tq, eq3), method = "lm") +
  labs(x = "theoretical Quantiles", y = "Sample Quantiles")

y <- rt(200, df = 5)
qqnorm(y, main = "Normal Q-Q Plot",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles",
       plot.it = TRUE, datax = FALSE, )
```


# Lecture 3

## Expected values
E[X] =sum xp(x)
E[g(X)] =Sum g(x)p(x)
Suppose that a die is tossed and X is the number face up
â€¢ What is the expected value of X?
```{r expected value}
# Simulate the mean of n die rolls
mx5 <- rep(0, 1000)
mx10 <- rep(0, 1000) 
mx20 <- rep(0, 1000)
mx100 <- rep(0,1000)

for (i in 1:1000) {
  mx5[i] <- mean(sample(1:6, 5, replace = TRUE)) # 5 die rolls
  mx10[i] <- mean(sample(1:6, 10, replace = T))  # 10 die rolls
  mx20[i] <- mean(sample(1:6, 20, replace = T))  # 20 die rolls
  mx100[i] <- mean(sample(1:6, 100, replace = T)) # 100 die rolls
}


die_roll <- data.frame(mx5, mx10, mx20, mx100)
ggplot(die_roll) +
  geom_histogram(aes(mx5))

ggplot(die_roll) +
  geom_histogram(aes(mx100))
```

### continuous rndom variable with pdf f(x),
Expected value = $ \int_x xf(x) dx $

```{r uniform distribution}
y1<-mean(runif(10000))
y2<-mean(runif(10000,0,10))
y3<-mean(runif(10000,-2,15))
```

```{r Gamma distribution}


```


# Lecture 4
### Jensen's inequality
E[h(x)] >= h(E[x]) if h(x) is convex function (holds water)

### Variance
var(x) = E[(x-Î¼)2]  = E[x2]-(E[x])2

```{r variabce}
x <- seq(-10, 10, length = 201)
y1 <- dnorm(x)
y2 <- dnorm(x, 0, 2)
y3 <- dnorm(x, 0, 4)
plot(x,y1, type = "l", col = "red", lwd = 3)
lines(x,y2,col="orange",lwd=3)
lines(x,y3,col="blue",lwd=3)

# Calculating the sample variance in R
y <- rnorm(100, 0, 4)
var(y)
sd(y)
mean(y)
```

## Chebyshev's inequality
P(|x-Î¼|>=kÏƒ) <= 1/k2

## Independent events
P(A unit B) = P(A)P(B)
Joint Distribution under Independence: f(x1,x2,...,xn) = ç´¯ä¹˜fi(xi)

## IID random variables
In the instance where f1 = f2 = ... = fn we say that the Xi are iid for independent and identically distributed 
```{r simulating independent discrete variable}
# IID Bernoulli draws
x <- rbinom(20, 1, prob = 0.3)

# Independent Bernoulli draws (not indentically distributed)
bernm<-seq(0,1,by=0.2)
x<-rbinom(20*length(bernm),1,prob=bernm)


# Independent Poisson draws
x<-rpois(10000,20)
# Independent Poisson draws (not indentically distributed)
poism<-c(1,2.5,5,7.5,10,1000)
x<-rpois(10*length(poism), poism)

# IID Normal draws
x<-rnorm(1000,mean=2,sd=9)
# Independent (but not identical) Normal draws
normm<-1:100
sdm<-normm/3
x<-rnorm(150*length(normm),mean=normm,sd=sdm)

```

# joint density for random vector
The standard normal density is N(0,1)
Suppose that one draws n independent samples, X1,...Xn from a distribution with the pdf given above
 What is the joint density of the vector (X1,...Xn)?
```{r iid joint density}
# Let's suppose that n = 2 (one makes two independent draws from a standard normal)
# What is P(X1>=1.5,X2 >=1)?
probt=(1-pnorm(1.5))*(1-pnorm(1))
probt
nsim=100000
x1=rnorm(nsim)
x2=rnorm(nsim)
probs=mean((x1>1.5) & (x2>1))
probt # display theoretical value
probs # display simulated value
abs(probs-probt)/probt
```
Monte Carlo methods are incredibly powerful for evaluating probabilities.


# Lecture 6
## likelihood function
Normal distribution
```{r normal}
bx=c(5,2,3) # this is the data
mu=seq(0,6,length=201) # parameter values
likep=rep(0,201)
for (i in 1:201)
febx2=-sum((bx-mu[i])^2)/2
likep[i]=exp(ebx2)/((2*pi)^(length(bx)/2))g
plot(mu,likep,type="l",col="blue",lwd=3)
mle<-mu[which.max(likep)]

```


```{r lecture 6}
theta=seq(1,10,by=0.1)
like=1/theta^3*(theta>=5)
plot(theta,like,type="l",col="blue",lwd=3)


like[theta==6]/like[theta==5]
like[theta==6]/like[theta==4]
theta[which.max(like)] # maximum likelihood
liken=like/max(like)
plot(theta,liken,type="l",col="blue",lwd=3)



```


# Lecture 7

```{r binomial}
# Simulate 15 independent Binomial(10,.2)
rbinom(15,size=10,prob=0.2)

# Simulate 15 independent Binomial(10,p)
# First p=0.1, last p=0.9
rbinom(15,size=10,prob=seq(0.1,0.9,length=15))

# Simulate 15 independent Binomial(n,p)
# First (n,p)=(1,0.1), last (n,p)=(15,0.9)
rbinom(15,size=1:15,prob=seq(0.1,0.9,length=15))

```


```{r normal distribution}
# Probability of at most 1 deviation
pnorm(1)-pnorm(-1)
# Probability of at most 2 deviation
pnorm(2)-pnorm(-2)
# Probability of at most 3 deviation
pnorm(3)-pnorm(-3)

```



# Lecture 8 Law of Large Numbers, Central Limit Theorem
Generating sequences of means:
n -> unlimited, so P( |(xn)Ì… -Î¼| >Îµ ) â‰¤ 0
```{r Limit of sequence}
x1=rbinom(1000,1,0.5)
xbar1=cumsum(x1)/(1:1000)
y1 = xbar1 - mean(x1)
plot(1:1000, y1, type = "l", col = "blue",
     xlab = "Number of trials", ylab = "Distance to the mean",
     main = "Sequences of Sample Means")
```

## convergence of transformed data
Does WLLN work for Cauchy?
```{r Cauchy}
n <- 10000
mu <- 3
x <- rcauchy( n, mu )
cmean <- cumsum(x)/(1:n)
plot(1:n, cmean, type="l")
abline(h=mu)

```

# central limit theorem
Assume that X1, . . . ,Xn are iid with an exp(1) distribution
f (x) = exp(âˆ’x) for x > 0
â€¢ E[Xi ] = 1, Var(X) = 1
â€¢ Let Xnæ‹” = 1/nÎ£Xi
â€¢ Simulate Xnæ‹” for n = 3, n = 30 and plot
â€¢ Show histograms of Xnæ‹” and Zn

```{r simulation exponential}
x <- seq(0,5,length=101)
h <- dexp(x, rate = 1)
n = c(3, 30)
mx = matrix(rep(0,2000), ncol = 2)

for (i in 1:1000) {
  mx[i,1] = mean(rexp(n[1], rate = 1))
  mx[i,2] = mean(rexp(n[2], rate = 1))
}

plot(x, h, type = "l", col = "lightblue", lwd = 3,
     ylim = c(0,2.5))
hist(mx[,1],prob = T, add = T, col=rgb(0,0,1,1/4), breaks = 25)
hist(mx[,2],prob = T, add = T, col=rgb(1,0,1,1/4), breaks = 25)

```

### R simulations: exponential Z-score
Zn = (Xnæ‹”-1)/(1/sqrt(n)) = sqrt(n)*(xnæ‹”-1)
```{r z-normalized}
zx= mx 
for (j in 1:2) {
  zx[,j]<-sqrt(n[j])*(mx[,j]-1)
  }
xx=seq(-3,3,length=101)
yx=dnorm(xx)
plot(xx,yx,type="l",col="blue",lwd=3)
hist(zx[,1],prob=T,add=T,col=rgb(0,0,1,1/4),breaks=50)
hist(zx[,2],prob=T,add=T,col=rgb(1,0,0,1/4),breaks=50)
```



# Lecture 9
```{r}
##quantiles of a chi-square distribution
df = 4
alpha <- .05
qchisq(c(alpha/2, 1 - alpha/2), df)

qchisq(0.025, df=4)  
qchisq(0.975, df=4)

qchisq(0.025, df=512)  
qchisq(0.975, df=512)
```

A recent study *513* of organo-lead manufacturing workers reported an average total brain volume of 1, 150.315cm3 with a *standard deviation* of 105.977. Assuming normality of the underlying measurements, calculate a confidence interval for the population variation in total brain volume
```{r}
n <- 513
miu <- 1150.315
s2 <- 105.977^2
a <- 0.05
quantile <- qchisq( c(a/2, 1-a/2), df = n-1  )
CI <- c( (n-1)*s2/quantile[1], (n-1)*s2/quantile[2] )
CI
(n-1)*s2/rev(quantile)
```


(n-1)S^2 follows a gamma distribution with shape (n-1)/2 and scale Ïƒ2/2
Therefore, this can be used to plot a likelihood function for Ïƒ2
```{r likelihood sigma}
n <- 100
s2 <- 16
sigmaVals <- seq(5, 15, length = 1000)
likeVals <- dgamma((n - 1) * s2, shape = (n - 1)/2, scale = sigmaVals^2/2)
likeVals <- likeVals / max(likeVals)
plot(sigmaVals, likeVals, type = "l")
lines(range(sigmaVals[likeVals >= 1 / 8]),
c(1 / 8, 1 / 8))
lines(range(sigmaVals[likeVals >= 1 / 16]),
c(1 / 16, 1 / 16))
```

# t quantiles
```{r}
##quantiles of a t distribution
n=c(1,2,5,10)
alpha <- .05
c(qt(1-alpha/2,n),qnorm(1-alpha/2))
qt(1-alpha/2,n)
qnorm(1-alpha/2)
```
results start to close to normal distribution, wiht the df increases


```{r example t distribution}
n <- 16
alpha <- 0.05
qt(1-alpha,n)

qt(1-alpha/2,n)
?qf
```
In R typing data(sleep) brings up the sleep data originally
analyzed in Gossetâ€™s Biometrika paper, which shows the
increase in hours of sleep for 10 patients on two soporific drugs.
R treats the data as two groups rather than paired.
```{r}
alpha <- 0.05
qt(1-alpha/2, df= 10)
1.67+ 2.228139*1.13/sqrt(10)
1.67- 2.228139*1.13/sqrt(10)

data(sleep)
g1 <- sleep$extra[1 : 10]
g2 <- sleep$extra[11 : 20]
difference <- g2 - g1
mn <- mean(difference)#1.67
s <- sd(difference)#1.13
n <- 10
mn + qt(c(.025, .975), n-1) * s / sqrt(n)
t.test(difference)$conf.int
[1] 0.7001142 2.4598858
# excludes 0
```

## the non-central t distribution

non-central t~âˆšn*Xba/S with non-centrality parameterâˆšnÎ¼/Ïƒ

```{r}
n<-1000
s = 10
mn = 20
tStat <- sqrt(n)*mn / s
esVals <- seq(0,1,length=1000)
likeVals <- dt(tStat,n-1,ncp = sqrt(n)*esVals)
nom_likeVals <- likeVals/max(likeVals)
plot(esVals,likeVals,type = "1")
lines(range(esVals[nom_likeVals>1/8]),c(1/8,1/8) )

```


# Lecture 10
# Independent group t confidence intervals
1. Independent 
2. equal varince
pooled variance estimator: is a good estimater of Ïƒ2
Sp^2 = {(nx-1)Sx^2 + (ny-1)Sy^2}/(nx+ny-2)
We know that (n âˆ’ 1)S^2/Ïƒ2 âˆ¼ Ï‡2(nâˆ’1)
So (nx + ny âˆ’ 2)Sp^2/Ïƒ2  âˆ¼ Ï‡2(nx + ny -2)

```{r t test}
# Confidence interval
alpha = 0.05
nx = 10
ny = 10
Sx = 9.5
Sy = 11.5
Î¼x = 59.6
Î¼y =  63.5
Sp =sqrt( ((nx-1)*Sx^2 + (ny-1)*Sy^2) /(nx + ny -2) )
t = qt(1-alpha/2, nx+ny-2)
CI <- c((Î¼x-Î¼y)+t*Sp*sqrt(1/nx+1/ny),   (Î¼x-Î¼y)- t*Sp*sqrt(1/nx+1/ny))
 CI 
```

## P value
```{r}
Tobs <- (Î¼y-Î¼x)/(Sp*sqrt(1/nx+1/ny))
Tobs
Pvalue <- 2*pt(-Tobs, 18)
```



## Unequal vriance
two options:
â€¢ Test whether Ïƒ2x = Ïƒ2y ; apply the previous methods if this hypothesis is not rejected
â€¢ Develop a new test which does not assume Ïƒ2x = Ïƒ2y
```{r}
ux = 132.6
uy = 127.4
Sx = 15.34
Sy = 18.23
nx = 8
ny = 21
point_estimate <- (Sx^2/(nx-1))/(Sy^2/(ny-1))

qf(1-alpha/2,nx-1,ny-1)
Sx^2/Sy^2 * qf(1-alpha/2,nx-1,ny-1)
Sx^2/Sy^2 * qf(alpha/2,nx-1,ny-1)
```

### General formula
t.test(x, ...)
#### Default S3 method:
t.test(x, y = NULL,
alternative = c("two.sided","less","greater"),
mu = 0, paired = FALSE, var.equal = FALSE,
conf.level = 0.95, ...)

â€¢ One sample t-test (e.g. H0 : Î¼ = u0 )
â€¢ Two sample t-test (e.g. H0 : Î¼X = Î¼Y )
â€¢ Alternative hypotheses (e.g. HA : Î¼X Ì¸= Î¼Y or HA : Î¼X < Î¼Y or HA : Î¼X > Î¼Y )

```{r One sample t-test}
x<-1:10
t.test(x,alternative="two.sided",mu=2)
```

```{r Two sample t-test: equal variance}
x<-1:10
y<-c(7:20)
t.test(x,y,alternative="less",var.equal = TRUE)
```

### Unequal variance: Welch Two Sample t-test
```{r Two sample t-test: unequal variance}
t.test(extra ~ group, data = sleep)
```
data: extra by group
t = -1.8608, df = 17.776, p-value = 0.07939
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval: -3.3654832 0.2054832
sample estimates:
mean in group 1 mean in group 2
  0.75                  2.33

### Paired two sample t-test
```{r Paired two sample t-test}
t.test(extra ~ group, paired=TRUE, data = sleep)
```
data: extra by group
t = -4.0621, df = 9, p-value = 0.002833
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:-2.4598858 -0.7001142
sample estimates:
mean of the differences
      -1.58
### Paired two sample t-test: Prior-Post 
```{r}
extra=sleep$extra
group=sleep$group
before=extra[group == 1]
after=extra[group == 2]
t.test(before, after, paired=TRUE)
```


# Lecture 11
## Normality
Assessing normality is both easy and hard
## 1. QQ plot
You plot theoretical normal quantile on the X-axis
You plot empirical data quantile on the Y-axis, which is
data sorted in ascending order: y(1), ..., y(n)  (y1 is the minimum one)

Unless something is egregiously eye-catching, we would assume normality
```{r qq plot}
data(faithful)
par(mfrow=c(1,2))
with(faithful, qqnorm(eruptions, main="Eruption duration") )
with(faithful, qqline(eruptions))
with(faithful, qqnorm(waiting, main="Waiting time (min)"))
with(faithful, qqline(waiting))
```

## 2. Kernel Density Plo
It only works when there is sufficient data (e.g., 30 or more)
Unless it is egregiously non-normal (e.g., skewed, bimodal,ceiling/floor), we would assume normality
â€¢ A key control parameter is *bandwidth*, which may need to be tweaked
```{r kernel density plot}
data(faithful)
par(mfrow=c(1,2))
plot(density(faithful$eruptions), main="Eruption duration")
plot(density(faithful$waiting), main="Waiting time (min)")
    
# Using a different bandwidth estimator
plot(density(faithful$eruptions, bw="SJ"), main="Eruption duration")
plot(density(faithful$waiting, bw="SJ"), main="Waiting time (min)")
```

## 3. Shapiro-Wilk Test
Most powerful (in a technical sense) and widely used test
â€¢ Sort the data: Y(1) < Y(2), ..., < Y(n)
Y(i) is the i-th order statistics
ai are obtained from expectations and covariance of {Y(1), Â· Â· Â· , Y(n)}
â€¢ 0 â‰¤ W â‰¤ 1; Null is rejected for smaller values of W
â€¢ close to 1: normal
(W is not percent, 0.9 does not means good fit to normal)
```{r Shapiro-Wilk test}
data(faithful)
shapiro.test(faithful$eruptions)

# t-dist with df=5 and 50
y <- matrix(rt(200, df=c(5,50)), 100, 2, byrow = TRUE)
apply(y, 2, shapiro.test)
```


# Lecture 12
## Negative Binomial distribution

X: x number of trails are needed to get n successes
n: number of successes;
X âˆ¼ NB(n, p)
Y: number of failures;
Y âˆ¼ NB(n, p)
P(X = x) = choose(x-1, n-1) * p^n *(1-p)^(x-n)
E(X) = n/p ; Var(X) = n(1âˆ’p)/p2

P(Y = y) = P(X = n + y) = choose(n+yâˆ’1, nâˆ’1)*p^n(1 âˆ’ p)^y

### pnbinom(q, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
q: vector of quantiles (y, number of failure)
size: number of successes ï¼ˆnï¼‰
prob: probability of success in each trail
```{r NB}
pnbinom(4,5,0.7)  # P(Y <= 4) , successes = 5, prob of success = 0.7

choose(8,4)*0.7^5*0.3^4  + choose(7,4)*0.7^5*0.3^3 + choose(6,4)*0.7^5*0.3^2 + choose(5,4)*0.7^5*0.3^1 + choose(4,4)*0.7^5*0.3^0
```
## P-value and Significance Levels of Negative Binormal
## Generalized Likelihood Ratio method
Finding the Critical Region for NB(n, p0)
```{r}
NB_LR.k <- function(x, n, k){
# compute the log-likelihood ratio: LLR = log(L(p=0.5)/L_max)
# also compute LLR - log(k) so we can find critical region where LR <= 
  xlgx <- ifelse(x==0, 0, x*log(x))
- log(k) + (n+x)*(log(n+x) - log(2*n)) - xlgx + x*log(n)
}

cr.nb <- function(n, kvals=1/(5:10)){
crmat <- matrix(NA, length(kvals), 4)
i <- 0
for (k in kvals){
i <- i+1
xmin <- nleqslv(1, NB_LR.k, n=n, k=k)$x
xmax <- nleqslv(n+1, NB_LR.k, n=n, k=k)$x
cr <- c(floor(xmin), ceiling(xmax)) # integers
alpha <- pnbinom(cr[1],n,0.5) + pnbinom(cr[2]-1,n,0.5, lower=FALSE)
crmat[i,] <- c(signif(k,3), cr, signif(alpha,3))
}
colnames(crmat) <- c("1/k", "cr.lower", "cr.upper", "sig-Level")
return(crmat)
}
```

### Critical Region for NB(n = 17, p0 = 0.5)
```{r}
# install.packages("nleqslv")
library(nleqslv)
n <- 17
kvals <- 1/(5:9)
cr.nb(n, kvals) # critical regions for different k
```

### Comparison with qnbinom()
```{r}
soln <- c(qnbinom(.025,n,0.5), qnbinom(.975,n,0.5))
print(soln)
# install.packages("sig.level")
library(sig.level)
sig.level(soln, n, 0.5) # significance level > 0.05!
```










