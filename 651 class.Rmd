---
title: "651 class"
author: "Siyu Zou"
date: "2023-09-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
tq<-qexp(seq(0.01,0.99,length=100), 1/5)
plot(tq)

union(x,y) #æ±‚å¹¶é›†

intersect(x,y) #æ±‚äº¤é›†

setdiff(x,y) #æ±‚å±äºxè€Œä¸å±äºyçš„æ‰€æœ‰å…ƒç´ 

setequal(x,y) #åˆ¤æ–­xä¸yæ˜¯å¦ç›¸ç­‰

a %in% y #åˆ¤æ–­aæ˜¯å¦ä¸ºyä¸­çš„å…ƒç´ 

choose(n, k) #nä¸ªé‡Œé¢å–kä¸ªçš„ç»„åˆæ•°

combn(x,n) #xä¸­çš„å…ƒç´ æ¯æ¬¡å–nä¸ªçš„æ‰€æœ‰ç»„åˆ

combn(x,n,f) #å°†è¿™äº›ç»„åˆç”¨äºæŒ‡å®šå‡½æ•°f
```

# Lecture 1 & 2
## Probability calculus
```{r birthday problem}
n=1:364
pn=n
for (i in 1:364) {
  pn[i]<- 1-prod(365:(365-i+1))/365^i
  }

pn[23]  # 50%

pn[57]  # 99%
```

```{r ploting birthday problem}
plot(
  n[1:100],
  pn[1:100],
  type = "p",
  pch = 19, 
  col = "red",
  lwd = 3,
  xlab = "Number of people in the room",
  ylab = "Probability of at least two same-day birthdays",
  cex.lab = 1.5, cex.axis = 1.5, col.axis = "black"
)


```


### sample
```{r Monte Holl problem}

# help on sample
?sample
# a random permutation
x <- sample (1:6)  #  the default for size == the first argument, so that sample(x) generates a random permutation of the elements of x (or 1:x).

# sampling with replacement
x <- sample ( 1:6, 10, replace=T )  # size = 10

# how many are equal to 3?
sum ( x == 3 )

# nonparametric bootstrap
x <- sample ( 1:10, 10, replace=T )

# 100 Bernoulli trials
sample(c(0,1), 100, replace = TRUE)

```

### Can you write an R code to simulate the Monte Hall problem?
the answer is in Lab 2 notes


## Probability mass function (PMF) for discrete variable

### Bernoulli & Binomial (sum of Bernoulli)
```{r Bernoulli example}
?rbinom
# rbinom(n, size, prob)
# n: number of observations. If length(n) >1, the length is taken to be the number required.
# size: number of trails (0 or more)
# Let X be the result of a single coin flip where X = 0 represents tails and X = 1 represents heads

# Bernoulli: 1æ¬¡å®éªŒ
rbinom(1,1,0.5)
rbinom(20, 1, 0.3) #  1ä¸ªç¡¬å¸(size), æŠ›20æ¬¡(n)ï¼Œæ¯æ¬¡æ¦‚ç‡0.3 (p)
rbinom(20,1,seq(0,1,length=20))  #  1ä¸ªç¡¬å¸(size), æŠ›20æ¬¡(n)ï¼Œæ¯æ¬¡æ¦‚ç‡ ä»0åˆ°1å¢åŠ  (p)

# Binomial : n æ¬¡Bernoulliå®éªŒï¼Œ sizeï¼šæ€»å…±å¤šå°‘ä¸ªç¡¬å¸
# Simulate 15 independent Binomial(10,.2)
rbinom(15,size=10,prob=0.2)  # 10ä¸ªç¡¬å¸ï¼ˆsizeï¼‰ï¼ŒæŠ›15æ¬¡ï¼ˆnï¼‰ï¼Œæ¯æ¬¡æ¦‚ç‡0.2
# Simulate 15 independent Binomial(10,p)
# First p=0.1, last p=0.9
rbinom(15,size=10,prob=seq(0.1,0.9,length=15))
# Simulate 15 independent Binomial(n,p)
# First (n,p)=(1,0.1), last (n,p)=(15,0.9)
rbinom(15,size=1:15,prob=seq(0.1,0.9,length=15))  # 1-15ä¸ªç¡¬å¸ï¼ˆsizeï¼‰ï¼ŒæŠ›15æ¬¡ï¼ˆnï¼‰ï¼Œæ¯æ¬¡æ¦‚ç‡0.1-0.9
```

Suppose a man has 8 children, 7 of which are girls and none are twins
 If each gender has an independent 50% probability for each birth, what's the probability of getting 7 or more girls out of 8 births?
```{r}
pbinom(6, 8, 0.5, lower.tail = FALSE)
```
This calculation is an example of a p-value: the probability, under the null hypothesis, of getting a result as extreme or more extreme than the one actually obtained. 



### Poisson
```{r Poisson example}
x <- 1:100
lambda = 20
plot(x, dpois(x, lambda),type="p",pch=19,
col="blue",lwd=3, xlab="Number of patients",
ylab="Probability",cex.lab=1.5,cex.axis=1.5,
col.axis="blue")


?dpois

# sampling the Poisson distribution 
# A sample of 15 independent days with average number of patients = 20
rpois(15, lambda=20)

# Reconstructing PMF from samples
y <- rpois(10000, lambda=20)
py=rep(0,100)
for (i in 1:100) {
  py[i] <- sum(y==i)/10000 
}

# Add lines to the PMF plot of the Poisson distribution
plot(x, dpois(x, lambda),type="p",pch=19,
col="blue",lwd=3, xlab="Number of patients",
ylab="Probability",cex.lab=1.5,cex.axis=1.5,
col.axis="blue") + 
lines(1:100,py,col="red",lwd=3)

```


## Probability density function for continuous variable
Whatâ€™s the probability that a randomly selected person from this distribution survives more than 6 years?
f(x) = (e^-x/5)/5
P(X>=6) = $\int_{0}^{Inf}e^-t/5 /5 dt$

```{r exponential}
pexp(6, 1/5, lower.tail = FALSE)
x <- 1:100
plot (dexp(x,1/5 ))
pexp(6, 1/5) - pexp(5, 1/5)
```

Double exponential
```{r double exponential}
x <- seq(-5,5,length(101))
fx <- exp(-abs(x))/2
plot(x, fx, type = "l", col = "lightblue", lwd = 3,
     xlab = "error", ylab = "PDF", cex.lab = 1.5,
     cex.axis = 1.5, col.axis = "blue"
     )
```
consider a more general version of f(.)
f(x) = $1/2Ïƒ e^(-|x-Î¼|/Ïƒ)$
is this a valid density?
Plotting double exponentials with mean Î¼ = 2 and Ïƒ = 1, 2, 4
```{r double exponentials Î¼ = 2}
# construct the empirical pdf from 100 simulated observation
mu <- 2
sigma = 2
# install.packages("vgam")
# library("vgam")
y = replace(100, mu, sigma)
hist(y, breaks = 20, probability = TRUE, xlab = "error")

# Super impose the theoretical pdf
x=seq(mu-7,mu+7,length=101)
fx=exp(-abs(x-mu)/sigma)/(2*sigma)
plot(x,fx,col="orange",lwd=3)

```

# CDF Cumulative distribution function

# Quantile
What is the 25th percentile of the exponential survival distribution considered before??
â€¢ We want to solve (for x)
.25 = F(x)
= 1 âˆ’ eâˆ’x/5
resulting in the solution x = âˆ’log(.75) Ã— 5 â‰ˆ 1.44
â€¢ Therefore, 25% of the subjects from this population live less than 1.44 years
```{r quantile}
# R can approximate exponential quantiles for you
qexp( p = 0.25, 1/5)  # quartile
qexp( p = 1/3, 1/5)  # tertile
```


# QQ plot
Calculate the quantiles of an exp(5) distribution
```{r qq}
tq<-qexp(seq(0.01,0.99,length=100), 1/5)
plot(tq)
```

How do they compare to the empirical quantiles?
```{r }
x1<-rexp(30, 1/5)
x2<-rexp(30, 1/5)
x3<-rexp(30, 1/5)
eq1<-quantile(x1,seq(0.01,0.99,length=100))
eq2<-quantile(x2,seq(0.01,0.99,length=100))
eq3<-quantile(x3,seq(0.01,0.99,length=100))
```

Plot theoretical versus empirical quantiles (QQ plots)
```{r qq plot}
library(ggplot2)
data <- data.frame( eq1, eq2, eq3, tq )

library(tidyverse)
data %>%
  ggplot() +
  geom_point(aes(tq, eq1 )) +
  geom_point(aes(tq,eq2),method = "lm" ) +
  geom_point(aes(tq, eq3), method = "lm") +
  labs(x = "theoretical Quantiles", y = "Sample Quantiles")

y <- rt(200, df = 5)
qqnorm(y, main = "Normal Q-Q Plot",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles",
       plot.it = TRUE, datax = FALSE, )
```


# Lecture 3

## Expected values
E[X] =sum xp(x)
E[g(X)] =Sum g(x)p(x)
Suppose that a die is tossed and X is the number face up
â€¢ What is the expected value of X?
```{r expected value}
# Simulate the mean of n die rolls
mx5 <- rep(0, 1000)
mx10 <- rep(0, 1000) 
mx20 <- rep(0, 1000)
mx100 <- rep(0,1000)

for (i in 1:1000) {
  mx5[i] <- mean(sample(1:6, 5, replace = TRUE)) # 5 die rolls
  mx10[i] <- mean(sample(1:6, 10, replace = T))  # 10 die rolls
  mx20[i] <- mean(sample(1:6, 20, replace = T))  # 20 die rolls
  mx100[i] <- mean(sample(1:6, 100, replace = T)) # 100 die rolls
}


die_roll <- data.frame(mx5, mx10, mx20, mx100)
ggplot(die_roll) +
  geom_histogram(aes(mx5))

ggplot(die_roll) +
  geom_histogram(aes(mx100))
```

### continuous rndom variable with pdf f(x),
Expected value = $ \int_x xf(x) dx $

```{r uniform distribution}
y1<-mean(runif(10000))
y2<-mean(runif(10000,0,10))
y3<-mean(runif(10000,-2,15))
```

```{r Gamma distribution}


```


# Lecture 4
### Jensen's inequality
E[h(x)] >= h(E[x]) if h(x) is convex function (holds water)

### Variance
var(x) = E[(x-Î¼)2]  = E[x2]-(E[x])2

```{r variabce}
x <- seq(-10, 10, length = 201)
y1 <- dnorm(x)
y2 <- dnorm(x, 0, 2)
y3 <- dnorm(x, 0, 4)
plot(x,y1, type = "l", col = "red", lwd = 3)
lines(x,y2,col="orange",lwd=3)
lines(x,y3,col="blue",lwd=3)

# Calculating the sample variance in R
y <- rnorm(100, 0, 4)
var(y)
sd(y)
mean(y)
```

## Chebyshev's inequality
P(|x-Î¼|>=kÏƒ) <= 1/k2

## Independent events
P(A unit B) = P(A)P(B)
Joint Distribution under Independence: f(x1,x2,...,xn) = ç´¯ä¹˜fi(xi)

## IID random variables
In the instance where f1 = f2 = ... = fn we say that the Xi are iid for independent and identically distributed 
```{r simulating independent discrete variable}
# IID Bernoulli draws
x <- rbinom(20, 1, prob = 0.3)

# Independent Bernoulli draws (not indentically distributed)
bernm<-seq(0,1,by=0.2)
x<-rbinom(20*length(bernm),1,prob=bernm)


# Independent Poisson draws
x<-rpois(10000,20)
# Independent Poisson draws (not indentically distributed)
poism<-c(1,2.5,5,7.5,10,1000)
x<-rpois(10*length(poism), poism)

# IID Normal draws
x<-rnorm(1000,mean=2,sd=9)
# Independent (but not identical) Normal draws
normm<-1:100
sdm<-normm/3
x<-rnorm(150*length(normm),mean=normm,sd=sdm)

```

# joint density for random vector
The standard normal density is N(0,1)
Suppose that one draws n independent samples, X1,...Xn from a distribution with the pdf given above
 What is the joint density of the vector (X1,...Xn)?
```{r iid joint density}
# Let's suppose that n = 2 (one makes two independent draws from a standard normal)
# What is P(X1>=1.5,X2 >=1)?
probt=(1-pnorm(1.5))*(1-pnorm(1))
probt
nsim=100000
x1=rnorm(nsim)
x2=rnorm(nsim)
probs=mean((x1>1.5) & (x2>1))
probt # display theoretical value
probs # display simulated value
abs(probs-probt)/probt
```
Monte Carlo methods are incredibly powerful for evaluating probabilities.


# Lecture 6
## likelihood function
Normal distribution
```{r normal}
bx=c(5,2,3) # this is the data
mu=seq(0,6,length=201) # parameter values
likep=rep(0,201)
for (i in 1:201)
febx2=-sum((bx-mu[i])^2)/2
likep[i]=exp(ebx2)/((2*pi)^(length(bx)/2))g
plot(mu,likep,type="l",col="blue",lwd=3)
mle<-mu[which.max(likep)]

```


```{r lecture 6}
theta=seq(1,10,by=0.1)
like=1/theta^3*(theta>=5)
plot(theta,like,type="l",col="blue",lwd=3)


like[theta==6]/like[theta==5]
like[theta==6]/like[theta==4]
theta[which.max(like)] # maximum likelihood
liken=like/max(like)
plot(theta,liken,type="l",col="blue",lwd=3)



```


# Lecture 7

```{r binomial}
# Simulate 15 independent Binomial(10,.2)
rbinom(15,size=10,prob=0.2)

# Simulate 15 independent Binomial(10,p)
# First p=0.1, last p=0.9
rbinom(15,size=10,prob=seq(0.1,0.9,length=15))

# Simulate 15 independent Binomial(n,p)
# First (n,p)=(1,0.1), last (n,p)=(15,0.9)
rbinom(15,size=1:15,prob=seq(0.1,0.9,length=15))

```


```{r normal distribution}
# Probability of at most 1 deviation
pnorm(1)-pnorm(-1)
# Probability of at most 2 deviation
pnorm(2)-pnorm(-2)
# Probability of at most 3 deviation
pnorm(3)-pnorm(-3)

```



# Lecture 8 








